{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBJECT RECOGNITION IN IMAGES USING CIFAR DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.core import fully_connected, dropout, input_data\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data\n",
    "CIFAR-10 Dataset<br>\n",
    "Credits: Alex Krizhevsky https://www.cs.toronto.edu/~kriz/cifar.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_1 = unpickle('cifar-10-batches-py/data_batch_1')\n",
    "dic_2 = unpickle('cifar-10-batches-py/data_batch_2')\n",
    "dic_3 = unpickle('cifar-10-batches-py/data_batch_3')\n",
    "dic_4 = unpickle('cifar-10-batches-py/data_batch_4')\n",
    "dic_5 = unpickle('cifar-10-batches-py/data_batch_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((dic_1[b'data'], dic_2[b'data'],dic_3[b'data'], dic_4[b'data'], dic_5[b'data']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.concatenate((dic_1[b'labels'], dic_2[b'labels'],dic_3[b'labels'], dic_4[b'labels'], dic_5[b'labels']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = unpickle('cifar-10-batches-py/batches.meta')[b'label_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting data into training and testing using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(data, labels, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "various method for data preprocessing, use either one of them<br>\n",
    "$Method$  $1$:<br> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1) First of all using dstack for stacking arrays in sequence depth wise as given dataset is in rgb format i.e. converting 3072 into three parts i.e. 1024 each for r,g and b colors, and normalizing data, by dividing by 255 as each bit ranges from 0-255 or can use standardscaler<br> \n",
    "$Method$ $2$:<br> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Similar to method one just after that we can convert rgb to just grayscale data, thus reducing shape from (?, 32, 32, 3) to (?, 32, 32), thus increasing overall speed<br> \n",
    "$Method$ $3$:<br> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Using PCA to speed up:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1) First we need to standardize the data using standard scalar, i.e. with mean of zero and a standard deviation of one.<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2) After standardization we pass it to PCA, with a variance of 0.95 which reduces 3072 features to just 221 whoes variance is greater than or equal to 0.95<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "train_x = train_x.astype(float)\n",
    "train_x = scalar.fit_transform(train_x)\n",
    "test_x = test_x.astype(float)\n",
    "test_x = scalar.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.dstack((train_x[:, :1024], train_x[:, 1024:2048], train_x[:, 2048:]))\n",
    "test_x = np.dstack((test_x[:, :1024], test_x[:, 1024:2048], test_x[:, 2048:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.reshape(train_x, [-1, 32, 32, 3])\n",
    "test_x = np.reshape(test_x, [-1, 32, 32, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x = train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.dstack((train_x[:, :1024], train_x[:, 1024:2048], train_x[:, 2048:])) / 255\n",
    "test_x = np.dstack((test_x[:, :1024], test_x[:, 1024:2048], test_x[:, 2048:])) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.reshape(train_x, [-1, 32, 32, 3])\n",
    "test_x = np.reshape(test_x, [-1, 32, 32, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "gray = rgb2gray(train_x)\n",
    "gray_test = rgb2gray(test_x)\n",
    "gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(2, 10, figsize=(10, 3))\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    a[0][i].imshow(gray[i])\n",
    "    a[1][i].imshow(train_x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x = gray\n",
    "test_x = gray_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "train_x = train_x.astype(float)\n",
    "train_x = scalar.fit_transform(train_x)\n",
    "test_x = test_x.astype(float)\n",
    "test_x = scalar.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x = pca.transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### converting output number to one hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot = pd.get_dummies(train_y).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can try various options for optimization of hyperparameters for better accuracy<br>\n",
    "1) learning rate<br>\n",
    "2) batch_size<br>\n",
    "3) n_epoch<br>\n",
    "4) one layer cnn<br>\n",
    "5) multiple layer cnn<br>\n",
    "6) dropout<br>\n",
    "7) image augmentation<br>\n",
    "8) regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input shape needs to be adjusted based upon preprocessing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "fnn = input_data(shape=[None, 32, 32, 3])\n",
    "#fnn = tf.reshape(fnn,[-1,221,1])\n",
    "#fnn = tf.reshape(fnn, [-1, 16, 16, 4])\n",
    "#print (fnn.shape)\n",
    "fnn = conv_2d(fnn, 32, 4, activation = 'relu', regularizer='L2')\n",
    "fnn = max_pool_2d(fnn, 2)\n",
    "#fnn = local_response_normalization(fnn)\n",
    "#fnn = conv_2d(fnn, 64, 4, activation = 'relu', regularizer='L2')\n",
    "#fnn = max_pool_2d(fnn, 2)\n",
    "#fnn = fully_connected(fnn,442, activation='relu')\n",
    "fnn = dropout(fnn, 0.1)\n",
    "fnn = fully_connected(fnn,256, activation='relu')\n",
    "fnn = fully_connected(fnn, 10, activation='softmax')\n",
    "fnn = regression(fnn, loss='categorical_crossentropy', learning_rate = 0.001, optimizer = 'adam')\n",
    "model = tflearn.DNN(fnn)\n",
    "model.fit(np.asarray(tr_x), np.asarray(y_train_onehot), n_epoch=100, batch_size=100, show_metric=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_x)\n",
    "pre = tf.Session().run(tf.argmax(predict,1))\n",
    "np.sum(pre==test_y)/len(test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
